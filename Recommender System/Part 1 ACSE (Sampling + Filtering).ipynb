{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d8cfe3e974008d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T21:15:42.133192Z",
     "start_time": "2025-04-15T21:15:42.039766Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import duckdb\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a92ac8e625e458",
   "metadata": {},
   "source": [
    "# 1. Sampling Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fab111-8fe5-4467-a75f-4c008b8bfbe2",
   "metadata": {},
   "source": [
    "## 1.1 Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd096d-8040-4cb2-904d-548dbfc0245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Connect with memory limit config\n",
    "# con = duckdb.connect(database=':memory:')\n",
    "\n",
    "\n",
    "# # Step 1: Segment customers and sample ~10%\n",
    "# con.execute(\"\"\"\n",
    "#     CREATE TABLE sampled_customers AS\n",
    "#     WITH customer_segments AS (\n",
    "#       SELECT \n",
    "#         store_id,\n",
    "#         cust_id,\n",
    "#         SUM(sales_amt) AS total_sales,\n",
    "#         CASE \n",
    "#           WHEN SUM(sales_amt) < 0 THEN 'negative_spender'\n",
    "#           WHEN SUM(sales_amt) < 50 THEN 'low_spender'\n",
    "#           WHEN SUM(sales_amt) < 100 THEN 'mid_spender'\n",
    "#           WHEN SUM(sales_amt) < 1000 THEN 'high_spender'\n",
    "#           ELSE 'vip'\n",
    "#         END AS spending_segment\n",
    "#       FROM read_parquet('combined.parquet')\n",
    "#       GROUP BY store_id, cust_id\n",
    "#     )\n",
    "#     SELECT * \n",
    "#     FROM customer_segments \n",
    "#     WHERE RANDOM() <= 0.10\n",
    "# \"\"\")\n",
    "\n",
    "# # Step 2: Join transactions with the sampled customers\n",
    "# con.execute(\"\"\"\n",
    "#     CREATE TABLE sampled_transactions AS\n",
    "#     SELECT t.*\n",
    "#     FROM read_parquet('combined.parquet') t\n",
    "#     JOIN sampled_customers cs\n",
    "#       ON t.store_id = cs.store_id AND t.cust_id = cs.cust_id\n",
    "# \"\"\")\n",
    "\n",
    "# con.execute(\"COPY sampled_transactions TO 'sampled_combined.parquet' (FORMAT 'parquet')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbfe381-d158-451e-9f11-cb38ff4d5f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb9b6057e93341ef9f3f169ac0752898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sample = con.execute(\"SELECT * FROM sampled_transactions\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed9f07-c2b8-46ce-87b1-2ccf0c16ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57149354-e1fe-4b5d-aa66-c01e827e6d34",
   "metadata": {},
   "source": [
    "## 1.2 Distribution KS Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982a3dec-79ef-48e0-89f0-b1db6921e2b2",
   "metadata": {},
   "source": [
    "### 1.2.1 Sales Amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be84279-1911-4ea7-8b90-3a1cdac8d7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "df_all_sales_amt = con.execute(\"\"\"\n",
    "    SELECT sales_amt\n",
    "    FROM 'combined.parquet'\n",
    "\"\"\").fetchdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fedd8-7201-4bd5-9fdb-2e70f45bd600",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_test = ['sales_amt']  \n",
    "for col in columns_to_test:\n",
    "    # Drop NA values\n",
    "    col_all = df_all_sales_amt.dropna()\n",
    "    col_sample = df_sample[col].dropna()\n",
    "    \n",
    "    sample_size = 100_000\n",
    "    if len(col_all) > sample_size:\n",
    "        col_all = col_all.sample(sample_size, random_state=42)\n",
    "    if len(col_sample) > sample_size:\n",
    "        col_sample = col_sample.sample(sample_size, random_state=42)\n",
    "\n",
    "    ks_stat, p_val = ks_2samp(col_all, col_sample)\n",
    "    print(f\"Kolmogorov-Smirnov test for '{col}':\")\n",
    "    print(f\"  KS statistic: {ks_stat:.6f}\")\n",
    "    print(f\"  p-value:      {p_val:.6g}\")\n",
    "    print(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12038f1-69ff-4feb-bd3c-11fbc19c1557",
   "metadata": {},
   "source": [
    "### 1.2.2 Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea72779-fddd-46e0-9c5a-2785c81021ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cust = con.execute(\"\"\"\n",
    "    SELECT cust_id\n",
    "    FROM 'combined.parquet'\n",
    "\"\"\").fetchdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5261b-2c86-4296-aeb6-08b2b09c4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_all = df_all_cust['cust_id'].dropna()\n",
    "cust_sample = df_sample['cust_id'].dropna()\n",
    "\n",
    "ks_stat, p_val = ks_2samp(cust_all, cust_sample)\n",
    "\n",
    "print(\"Kolmogorov-Smirnov test for 'cust_id':\")\n",
    "print(f\"  KS statistic: {ks_stat:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f737e0c-414a-4d23-978b-c8143f0b4c90",
   "metadata": {},
   "source": [
    "### 1.2.3 Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202bcec-b3b8-4181-afe7-0f5ef7f47774",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_store = con.execute(\"\"\"\n",
    "    SELECT store_id\n",
    "    FROM 'combined.parquet'\n",
    "\"\"\").fetchdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0dccdf-4e4f-4216-96b1-420c0c1c63ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_test = ['store_id']  \n",
    "\n",
    "for col in columns_to_test:\n",
    "    col_all = df_all_store[col].dropna()\n",
    "    col_sample = df_sample[col].dropna()\n",
    "    \n",
    "    sample_size = 100_000\n",
    "    if len(col_all) > sample_size:\n",
    "        col_all = col_all.sample(sample_size, random_state=42)\n",
    "    if len(col_sample) > sample_size:\n",
    "        col_sample = col_sample.sample(sample_size, random_state=42)\n",
    "\n",
    "    ks_stat, p_val = ks_2samp(col_all, col_sample)\n",
    "    print(f\"Kolmogorov-Smirnov test for '{col}':\")\n",
    "    print(f\"  KS statistic: {ks_stat:.6f}\")\n",
    "    print(f\"  p-value:      {p_val:.6g}\")\n",
    "    print(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4290098-c2b9-4664-a8a8-1f503fd6a15f",
   "metadata": {},
   "source": [
    "### 1.2.4 Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ffc46-5e67-4c88-b272-e253bb87ae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "df_all_product = con.execute(\"\"\"\n",
    "    SELECT prod_category\n",
    "    FROM 'combined.parquet'\n",
    "\"\"\").fetchdf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e934d-5719-4ce3-9788-5cb55ac7493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_test = ['prod_category']  \n",
    "\n",
    "for col in columns_to_test:\n",
    "    col_all = df_all_product[col].dropna()\n",
    "    col_sample = df_sample[col].dropna()\n",
    "    \n",
    "    sample_size = 100_000\n",
    "    if len(col_all) > sample_size:\n",
    "        col_all = col_all.sample(sample_size, random_state=42)\n",
    "    if len(col_sample) > sample_size:\n",
    "        col_sample = col_sample.sample(sample_size, random_state=42)\n",
    "\n",
    "    ks_stat, p_val = ks_2samp(col_all, col_sample)\n",
    "    print(f\"Kolmogorov-Smirnov test for '{col}':\")\n",
    "    print(f\"  KS statistic: {ks_stat:.6f}\")\n",
    "    print(f\"  p-value:      {p_val:.6g}\")\n",
    "    print(\"---------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b0a01b323fb3d2",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1c4fe2bb919d98",
   "metadata": {},
   "source": [
    "## 2.1 Load Samples and Observe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd234ce079b22440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T21:08:29.693865Z",
     "start_time": "2025-04-15T21:06:17.178768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the sampled transactions \n",
    "df_sampled = pd.read_parquet(\"sampled_combined.parquet\")\n",
    "print(\"Sampled transactions shape:\", df_sampled.shape)\n",
    "print(df_sampled.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d468c3c6fc00e5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T21:09:14.536471Z",
     "start_time": "2025-04-15T21:09:09.230042Z"
    }
   },
   "outputs": [],
   "source": [
    "## Understand the data structure \n",
    "# 1. Aggregate product frequencies from your sampled transactions data.\n",
    "product_counts = df_sampled['prod_desc'].value_counts()\n",
    "\n",
    "# Display basic statistics:\n",
    "print(\"Total unique products in sampled data:\", len(product_counts))\n",
    "print(\"\\nTop 10 most popular products:\")\n",
    "print(product_counts.head(10))\n",
    "\n",
    "# Calculate percentage of products purchased only once:\n",
    "single_purchase_count = (product_counts == 1).sum()\n",
    "print(f\"\\nProducts purchased only once: {single_purchase_count} \" +\n",
    "      f\"({100 * single_purchase_count / len(product_counts):.2f}%)\")\n",
    "\n",
    "\n",
    "# 2. Prepare the Data for Long Tail Plot\n",
    "\n",
    "# Sort the frequencies in descending order\n",
    "# product_counts is already sorted by .value_counts()\n",
    "sorted_counts = product_counts.values  # numpy array of counts, sorted descending\n",
    "ranks = np.arange(1, len(sorted_counts) + 1)\n",
    "\n",
    "# ---3. Plot on a Linear Scale\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(ranks, sorted_counts, color='blue', lw=2)\n",
    "plt.title(\"Product Frequency Long Tail (Linear Scale)\")\n",
    "plt.xlabel(\"Rank (Products sorted by frequency)\")\n",
    "plt.ylabel(\"Frequency (Number of Purchases)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ---4. Plot on a Log-Log Scale\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.loglog(ranks, sorted_counts, color='blue', lw=2)\n",
    "plt.title(\"Product Frequency Long Tail (Log-Log Scale)\")\n",
    "plt.xlabel(\"Rank (log scale)\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.grid(True, which=\"both\", ls=\"--\")\n",
    "plt.show()\n",
    "\n",
    "# --- 5. Cumulative Distribution Function (CDF) ---\n",
    "# Sort counts in descending order (again, already sorted in sorted_counts)\n",
    "cumulative_sum = np.cumsum(sorted_counts)\n",
    "total_purchases = cumulative_sum[-1]\n",
    "cumulative_percentage = (cumulative_sum / total_purchases) * 100\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(1, len(cumulative_percentage) + 1), cumulative_percentage, marker='o', linestyle='-')\n",
    "plt.xlabel(\"Rank of Product\")\n",
    "plt.ylabel(\"Cumulative Percentage of Total Purchases (%)\")\n",
    "plt.title(\"CDF of Product Purchases\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 6. 80/20 Analysis ---\n",
    "# Find how many products make up 80% of total purchases:\n",
    "# product_counts is a Series indexed by product, with total purchase counts\n",
    "product_counts = df_sampled.groupby('prod_id')['sales_qty'].sum()\n",
    "\n",
    "# compute percentages and cumulative percentages\n",
    "total_sales = product_counts.sum()\n",
    "percentage = (product_counts / total_sales) * 100\n",
    "cumulative_percentage = percentage.sort_values(ascending=False).cumsum()\n",
    "total_unique = cumulative_percentage.shape[0]\n",
    "\n",
    "idx_80 = np.where(cumulative_percentage >= 80)[0][0]\n",
    "num_products_80 = idx_80 + 1  # since index is zero-based\n",
    "perc_products_needed = 100 * num_products_80 / total_unique\n",
    "\n",
    "print(f\"\\nNumber of products to reach 80% of total purchases: {num_products_80}\")\n",
    "print(f\"Percentage of total products needed to reach 80% of sales: {perc_products_needed:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd10dbfdcc35752e",
   "metadata": {},
   "source": [
    "We have a Very Skewed Sample Dataset  SO WE WOULD WANT TO REMOVE THE LONGTAIL PRODUCTS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18837ca6b6dbd37",
   "metadata": {},
   "source": [
    "## 2.2 Remove longtail\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983f39851d9048f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T21:11:44.279295Z",
     "start_time": "2025-04-15T21:09:46.857307Z"
    }
   },
   "outputs": [],
   "source": [
    "## Remove longtail products + consumers to reduce dimensionality \n",
    "# === 1. Filter Products by Cumulative Sales (80% cutoff) ===\n",
    "# Aggregate sales by product ID\n",
    "product_sales = df_sampled.groupby('prod_id')['sales_qty'].sum()\n",
    "\n",
    "# Sort products in descending order based on sales\n",
    "product_sales = product_sales.sort_values(ascending=False)\n",
    "\n",
    "# Total sales quantity for all products\n",
    "total_sales = product_sales.sum()\n",
    "\n",
    "# Compute cumulative sales\n",
    "cumulative_sales = product_sales.cumsum()\n",
    "\n",
    "# Calculate cumulative percentage of sales for each product\n",
    "cumulative_percentage = (cumulative_sales / total_sales) * 100\n",
    "\n",
    "# Determine the cutoff index where cumulative sales reaches at least 80%\n",
    "threshold_index = np.where(cumulative_percentage >= 80)[0][0]\n",
    "\n",
    "# Select product IDs that contribute to 80% of the sales (include threshold index)\n",
    "selected_products = product_sales.index[:threshold_index + 1]\n",
    "\n",
    "print(\"Number of unique products before filtering:\", df_sampled['prod_id'].nunique())\n",
    "print(\"Number of products selected (top 80% cumulative sales):\", len(selected_products))\n",
    "\n",
    "# Filter the transactions to only include these top products\n",
    "df_filtered = df_sampled[df_sampled['prod_id'].isin(selected_products)]\n",
    "\n",
    "# === 2. Filter Customers by Transaction Count ===\n",
    "\n",
    "# Calculate the number of transactions per customer in the filtered data\n",
    "user_transactions = df_filtered['cust_id'].value_counts()\n",
    "\n",
    "# Define active customers as those with at least 3 transactions\n",
    "active_users = user_transactions[user_transactions >= 3].index\n",
    "\n",
    "# Filter the data to keep only the active customers\n",
    "df_filtered = df_filtered[df_filtered['cust_id'].isin(active_users)]\n",
    "\n",
    "print(\"Number of unique customers before filtering:\", df_sampled['cust_id'].nunique())\n",
    "print(\"Number of customers after filtering:\", df_filtered['cust_id'].nunique())\n",
    "\n",
    "# Optional: Display the shape of the filtered DataFrame\n",
    "print(\"Filtered DataFrame shape:\", df_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f6cadb80e2d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T21:13:27.565528Z",
     "start_time": "2025-04-15T21:11:46.948195Z"
    }
   },
   "outputs": [],
   "source": [
    "df_filtered.to_parquet(\"df_filtered.parquet\", index=False)\n",
    "print(\"Filtered dataset saved to df_filtered.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe299c7e895c4c1",
   "metadata": {},
   "source": [
    "# 3. DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14f4a02b94946",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-15T21:17:38.265699Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "df_filtered = pd.read_parquet(\"df_filtered.parquet\")\n",
    "print(\"Sampled transactions shape:\", df_sampled.shape)\n",
    "print(df_filtered.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c6dfd-d09b-4bef-bb82-5b824a8113a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix, save_npz\n",
    "\n",
    "#########################################\n",
    "# Part 2: Pre-Aggregation – Aggregate Purchases\n",
    "#########################################\n",
    "\n",
    "# Register df_sample with DuckDB (assuming df_sample is already defined)\n",
    "con.register(\"df_sample\", df_filtered)\n",
    "\n",
    "query_agg = \"\"\"\n",
    "SELECT \n",
    "  cust_id,\n",
    "  prod_desc,\n",
    "  COUNT(*) AS purchase_count\n",
    "FROM df_sample\n",
    "GROUP BY cust_id, prod_desc\n",
    "\"\"\"\n",
    "\n",
    "df_agg = con.execute(query_agg).fetchdf()\n",
    "print(\"New Aggregated data shape:\", df_agg.shape)\n",
    "print(df_agg.head())\n",
    "\n",
    "# Save aggregated data for later use\n",
    "df_agg.to_parquet(\"aggregated_data_new.parquet\", index=False)\n",
    "\n",
    "#########################################\n",
    "# Part 3: Building the Full Sparse User–Item Matrix\n",
    "#########################################\n",
    "\n",
    "# Instead of pivoting to a dense DataFrame, build a sparse matrix directly.\n",
    "# Step 1: Use LabelEncoder to convert cust_id and prod_desc into numeric indices.\n",
    "user_encoder = LabelEncoder()\n",
    "item_encoder = LabelEncoder()\n",
    "\n",
    "user_indices = user_encoder.fit_transform(df_agg['cust_id'])\n",
    "item_indices = item_encoder.fit_transform(df_agg['prod_desc'])\n",
    "\n",
    "# Step 2: Use the purchase_count as the data/values for the sparse matrix.\n",
    "values = df_agg['purchase_count'].astype(np.float32)\n",
    "\n",
    "# Step 3: Build the CSR sparse matrix.\n",
    "user_item_sparse = csr_matrix(\n",
    "    (values, (user_indices, item_indices)),\n",
    "    shape=(len(user_encoder.classes_), len(item_encoder.classes_))\n",
    ")\n",
    "\n",
    "print(\"Sparse matrix shape:\", user_item_sparse.shape)\n",
    "\n",
    "# Save the sparse matrix to disk\n",
    "save_npz(\"user_item_matrix_sparse_new.npz\", user_item_sparse)\n",
    "\n",
    "# Save the mappings so you can recover the original customer and product labels later.\n",
    "pd.DataFrame({'cust_id': user_encoder.classes_}).to_csv(\"user_id_mapping_new.csv\", index=False)\n",
    "pd.DataFrame({'prod_desc': item_encoder.classes_}).to_csv(\"item_id_mapping_new.csv\", index=False)\n",
    "\n",
    "#########################################\n",
    "# Part 4: Extract Product Metadata\n",
    "#########################################\n",
    "\n",
    "# Extract distinct product metadata columns.\n",
    "metadata_cols = ['prod_desc', 'prod_category', 'prod_subcategory', 'prod_mfc_brand_cd']\n",
    "product_metadata_df = df_sample[metadata_cols].drop_duplicates('prod_desc')\n",
    "print(\"Product metadata shape:\", product_metadata_df.shape)\n",
    "print(product_metadata_df.head())\n",
    "\n",
    "# Save product metadata for later use.\n",
    "product_metadata_df.to_parquet(\"product_metadata_new.parquet\", index=False)\n",
    "product_metadata_df.to_csv(\"product_metadata_new.csv\", index=False)\n",
    "\n",
    "#########################################\n",
    "# Final Print Summary\n",
    "#########################################\n",
    "print(\"\\n=== Preprocessing Complete ===\")\n",
    "print(\"1. NEW Sampled Data (df_sample_new) created.\")\n",
    "print(\"2. NEW Aggregated data saved to aggregated_data_new.parquet.\")\n",
    "print(\"3. NEW Sparse user–item matrix saved to user_item_matrix_sparse_new.npz.\")\n",
    "print(\"   (Mappings saved in  user_id_mapping_new.csv and  item_id_mapping_new.csv)\")\n",
    "print(\"4. Product Metadata saved to product_metadata_new.parquet and product_metadata_new.csv.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cc9844-25df-446a-a44e-8d0bf931ae29",
   "metadata": {},
   "source": [
    "## Brand Tagging\n",
    "This filters the rows that contain products that belongs to BUDW or MLSON Brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f8b47-8784-4587-b438-d2ea7e515d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_brands(data):\n",
    "    data[\"is_budweiser\"] = (\n",
    "        data[\"prod_desc\"].str.contains(\"bud\", case=False, na=False) |\n",
    "        data[\"prod_mfc_brand_cd\"].str.contains(\"BUDW\", na=False)\n",
    "    )\n",
    "    \n",
    "    data[\"is_molson\"] = (\n",
    "        data[\"prod_desc\"].str.contains(\"mlson|molson\", case=False, na=False) |\n",
    "        data[\"prod_mfc_brand_cd\"].str.contains(\"MLSON\", na=False)\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b9f6e7-fe4e-4986-b71b-d724eda20dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = tag_brands(df_agg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
